I"=R<blockquote>
  <p>Ceph 是一个符合POSIX、开源的分布式存储系统；其具备了极好的可靠性、统一性、鲁棒性；经过近几年的发展，Ceph 开辟了一个全新的数据存储途径。Ceph 具备了企业级存储的分布式、可大规模扩展、没有单点故障等特点，越来越受到人们青睐；以下记录了 Ceph 的相关学习笔记。</p>
</blockquote>

<h3 id="一-ceph-quick-start">一、 Ceph Quick Start</h3>

<h4 id="11安装前准备">1.1、安装前准备</h4>

<blockquote>
  <p>本文以 Centos 7 3.10 内核为基础环境，节点为 4 台 Vagrant 虚拟机；Ceph 版本为 Jewel.</p>
</blockquote>

<p>首先需要一台部署节点，这里使用的是宿主机；在部署节点上需要安装一些部署工具，如下</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 安装 EPEL 源</span>
yum <span class="nb">install</span> <span class="nt">-y</span> epel-release

<span class="c"># 添加 ceph 官方源</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; /etc/yum.repos.d/ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-jewel/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
</span><span class="no">EOF

</span><span class="c"># 安装部署工具</span>
yum update <span class="nt">-y</span> <span class="o">&amp;&amp;</span> yum <span class="nb">install </span>ceph-deploy <span class="nt">-y</span>
</code></pre></div></div>

<p><strong>同时，ceph-deploy 工具需要使用 ssh 来自动化部署 Ceph 各个组件，因此需要保证部署节点能够免密码登录待部署节点；最后，待部署节点最好加入到部署节点的 hosts 中，方便使用域名(某些地方强制)连接管理</strong></p>

<h4 id="12校对时钟">1.2、校对时钟</h4>

<p>由于 Ceph 采用 Paxos 算法保证数据一致性，所以安装前需要先保证各个节点时钟同步</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 安装 ntp 工具</span>
yum <span class="nb">install </span>ntp ntpdate ntp-doc <span class="nt">-y</span>
<span class="c"># 校对系统时钟</span>
ntpdate 0.cn.pool.ntp.org
</code></pre></div></div>

<h4 id="13创建集群配置">1.3、创建集群配置</h4>

<p>ceph-deploy 工具部署集群前需要创建一些集群配置信息，其保存在 <code class="highlighter-rouge">ceph.conf</code> 文件中，这个文件未来将会被复制到每个节点的 <code class="highlighter-rouge">/etc/ceph/ceph.conf</code></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建集群配置目录</span>
<span class="nb">mkdir </span>ceph-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd </span>ceph-cluster
<span class="c"># 创建 monitor-node</span>
ceph-deploy new docker1
<span class="c"># 追加 OSD 副本数量(测试虚拟机总共有3台)</span>
<span class="nb">echo</span> <span class="s2">"osd pool default size = 3"</span> <span class="o">&gt;&gt;</span> ceph.conf
</code></pre></div></div>

<h4 id="14创建集群">1.4、创建集群</h4>

<p>创建集群使用 ceph-deploy 工具即可</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 安装 ceph
ceph-deploy install docker1 docker2 docker3
# 初始化 monitor node 和 秘钥文件
ceph-deploy mon create-initial
# 在两个 osd 节点创建一个目录作为 osd 存储
mkdir /data
chown -R ceph:ceph /data
# 初始化 osd
ceph-deploy osd prepare docker1:/data docker2:/data docker3:/data
# 激活 osd
ceph-deploy osd activate docker1:/data docker2:/data docker3:/data
# 部署 ceph cli 工具和秘钥文件
ceph-deploy admin docker1 docker2 docker3
# 确保秘钥有读取权限
chmod +r /etc/ceph/ceph.client.admin.keyring
# 检测集群状态
ceph health
</code></pre></div></div>

<p>执行 <code class="highlighter-rouge">ceph health</code> 命令后应当返回 <code class="highlighter-rouge">HEALTH_OK</code>；如出现 <code class="highlighter-rouge">HEALTH_WARN clock skew detected on mon.docker2; Monitor clock skew detected</code>，说明时钟不同步，手动同步时钟稍等片刻后即可；其他错误可以通过如下命令重置集群重新部署</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph-deploy purge <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[{</span>ceph-node<span class="o">}]</span>
ceph-deploy purgedata <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[{</span>ceph-node<span class="o">}]</span>
ceph-deploy forgetkeys
</code></pre></div></div>

<p><strong>更多细节，如防火墙、SELinux配置等请参考 <a href="http://docs.ceph.com/docs/master/start/quick-start-preflight/#rhel-centos">官方文档</a></strong></p>

<h4 id="15其他组件创建">1.5、其他组件创建</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建 MDS</span>
ceph-deploy mds create docker1
<span class="c"># 创建 RGW</span>
ceph-deploy rgw create docker1
<span class="c"># 增加 monitor</span>
<span class="nb">echo</span> <span class="s2">"public network = 192.168.1.0/24"</span> <span class="o">&gt;&gt;</span> ceph.conf
ceph-deploy <span class="nt">--overwrite-conf</span> mon create docker2 docker3
<span class="c"># 查看仲裁信息</span>
ceph quorum_status <span class="nt">--format</span> json-pretty
</code></pre></div></div>

<h3 id="二ceph-组件及测试">二、Ceph 组件及测试</h3>

<h4 id="21ceph-架构图">2.1、Ceph 架构图</h4>

<p>以下图片(摘自网络)展示了基本的 Ceph 架构</p>

<p><img src="https://cdn.oss.link/markdown/o8gct.jpg" alt="ceph 架构" /></p>

<ul>
  <li>OSD: Ceph 实际存储数据单元被称为 OSD，OSD 可以使用某个物理机的目录、磁盘设备，甚至是 RAID 阵列；</li>
  <li>MON: 在 OSD 之上则分布着多个 MON(monitor)，Ceph 集群内组件的状态信息等被维护成一个个 map，而 MON 则负责维护集群所有组件 map 信息，各个集群内组件心跳请求 MON 以确保其 map 保持最新状态；当集群发生故障时，Ceph 将采用 Paxos 算法保证数据一致性，这其中仲裁等主要由 MON 完成，所以 MON 节点建议最少为 3 个，并且为奇数以防止脑裂情况的发生；</li>
  <li>MDS: Ceph 本身使用对象形式存储数据，而对于外部文件系统访问支持则提供了上层的 CephFS 接口；CephFS 作为文件系统接口则需要一些元数据，这些原数据就存放在 MDS 中；目前 Ceph 只支持单个 MDS 工作，<strong>但是可以通过设置 MDS 副本，以保证 MDS 的可靠性</strong></li>
  <li>RADOS: RADOS 全称 Reliable Autonomic Distributed Object Store，即可靠分布式对象存储；其作为在整个 Ceph 集群核心基础设施，向外部提供基本的数据操作</li>
  <li>librados: 为了支持私有云等程序调用，Ceph 提供了 C 实现的 API 库 librados，librados 可以支持主流编程语言直接调用，沟通 RADOS 完成数据存取等操作</li>
  <li>RBD: RDB 个人理解是一个命令行工具，一般位于宿主机上，通过该工具可以直接跟 librados 交互，实现创建存储对象，格式化 Ceph 块设备等操作</li>
  <li>RADOS GW: 从名字可以看出来，这个组件是一个代理网关，通过 RADOS GW 可以将 RADOS 响应转化为 HTTP 响应，同样可以将外部 HTTP 请求转化为 RADOS 调用；RADOS GW 主要提供了三大功能: <strong>兼容 S3 接口、兼容 Swift 接口、提供管理 RestFul API</strong></li>
</ul>

<p>下图(摘自网络)从应用角度描述了 Ceph 架构</p>

<p><img src="https://cdn.oss.link/markdown/fh5z8.jpg" alt="APP Ceph 架构" /></p>

<h4 id="22对象存储测试">2.2、对象存储测试</h4>

<p>此处直接上代码</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建测试文件</span>
<span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span><span class="nb">test </span><span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
<span class="c"># 创建对象存储池</span>
rados mkpool data
<span class="c"># 放入对象</span>
rados put test-file <span class="nb">test</span> <span class="nt">--pool</span><span class="o">=</span>data
<span class="c"># 检查存储池</span>
rados <span class="nt">-p</span> data <span class="nb">ls</span>
<span class="c"># 检查对象信息</span>
ceph osd map data test-file
<span class="c"># 删除对象</span>
rados <span class="nt">-p</span> data <span class="nb">rm </span>test-file
<span class="c"># 删除存储池(存储池写两遍并且加上确认)</span>
rados rmpool data data <span class="nt">--yes-i-really-really-mean-it</span>
</code></pre></div></div>

<h4 id="23块设备测试">2.3、块设备测试</h4>

<p><strong>官方文档中提示，使用 rdb 的客户端不建议与 OSD 等节点在同一台机器上</strong></p>

<blockquote>
  <p>You may use a virtual machine for your ceph-client node, but do not execute the following procedures on the same physical node as your Ceph Storage Cluster nodes (unless you use a VM). See FAQ for details.</p>
</blockquote>

<p>这里从第四台虚拟机上执行操作，首先安装所需客户端工具</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 部署节点上 ceph-cluster 目录下执行</span>
ceph-deploy <span class="nb">install </span>docker4
ceph-deploy admin docker4
</code></pre></div></div>

<p>然后创建块设备</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 块设备单位为 MB</span>
rbd create data <span class="nt">--size</span> 10240
<span class="c"># 映射块设备</span>
map foo <span class="nt">--name</span> client.admin
</code></pre></div></div>

<p><strong>在上面的 map 映射操作时，可能出现如下错误</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RBD image feature <span class="nb">set </span>mismatch. You can disable features unsupported by the kernel with <span class="s2">"rbd feature disable"</span>
</code></pre></div></div>

<p>查看系统日志可以看到如下输出</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ~ dmesg | <span class="nb">tail</span>
<span class="o">[</span><span class="nt">-1127592253</span>.530346] rbd: image data: image uses unsupported features: 0x38
<span class="o">[</span><span class="nt">-1127590337</span>.563180] libceph: mon0 192.168.1.11:6789 session established
<span class="o">[</span><span class="nt">-1127590337</span>.563741] libceph: client4200 fsid dd9fdfee-438a-47aa-be21-114372bc1f44
</code></pre></div></div>

<p><strong>问题原因: 在 Ceph 高版本进行 map image 时，默认 Ceph 在创建 image(上文 data)时会增加许多 features，这些 features 需要内核支持，在 Centos7 的内核上支持有限，所以需要手动关掉一些 features</strong></p>

<p>首先使用 <code class="highlighter-rouge">rbd info data</code> 命令列出创建的 image 的 features</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rbd image <span class="s1">'data'</span>:
        size 10240 MB <span class="k">in </span>2560 objects
        order 22 <span class="o">(</span>4096 kB objects<span class="o">)</span>
        block_name_prefix: rbd_data.37c6238e1f29
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        flags:
</code></pre></div></div>

<p>在 features 中我们可以看到默认开启了很多:</p>

<ul>
  <li>layering: 支持分层</li>
  <li>striping: 支持条带化 v2</li>
  <li>exclusive-lock: 支持独占锁</li>
  <li>object-map: 支持对象映射(依赖 exclusive-lock)</li>
  <li>fast-diff: 快速计算差异(依赖 object-map)</li>
  <li>deep-flatten: 支持快照扁平化操作</li>
  <li>journaling: 支持记录 IO 操作(依赖独占锁）</li>
</ul>

<p><strong>而实际上 Centos 7 的 3.10 内核只支持 layering… 所以我们要手动关闭一些 features，然后重新 map；如果想要一劳永逸，可以在 ceph.conf 中加入 <code class="highlighter-rouge">rbd_default_features = 1</code> 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)。</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 关闭不支持特性</span>
rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten
<span class="c"># 重新映射</span>
rbd map data <span class="nt">--name</span> client.admin
<span class="c"># 成功后返回设备位置</span>
/dev/rbd0
</code></pre></div></div>

<p><strong>最后我们便可以格式化正常挂载这个设备了</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>➜  ~ mkfs.xfs /dev/rbd0
meta-data<span class="o">=</span>/dev/rbd0              <span class="nv">isize</span><span class="o">=</span>512    <span class="nv">agcount</span><span class="o">=</span>17, <span class="nv">agsize</span><span class="o">=</span>162816 blks
         <span class="o">=</span>                       <span class="nv">sectsz</span><span class="o">=</span>512   <span class="nv">attr</span><span class="o">=</span>2, <span class="nv">projid32bit</span><span class="o">=</span>1
         <span class="o">=</span>                       <span class="nv">crc</span><span class="o">=</span>1        <span class="nv">finobt</span><span class="o">=</span>0, <span class="nv">sparse</span><span class="o">=</span>0
data     <span class="o">=</span>                       <span class="nv">bsize</span><span class="o">=</span>4096   <span class="nv">blocks</span><span class="o">=</span>2621440, <span class="nv">imaxpct</span><span class="o">=</span>25
         <span class="o">=</span>                       <span class="nv">sunit</span><span class="o">=</span>1024   <span class="nv">swidth</span><span class="o">=</span>1024 blks
naming   <span class="o">=</span>version 2              <span class="nv">bsize</span><span class="o">=</span>4096   ascii-ci<span class="o">=</span>0 <span class="nv">ftype</span><span class="o">=</span>1
log      <span class="o">=</span>internal log           <span class="nv">bsize</span><span class="o">=</span>4096   <span class="nv">blocks</span><span class="o">=</span>2560, <span class="nv">version</span><span class="o">=</span>2
         <span class="o">=</span>                       <span class="nv">sectsz</span><span class="o">=</span>512   <span class="nv">sunit</span><span class="o">=</span>8 blks, lazy-count<span class="o">=</span>1
realtime <span class="o">=</span>none                   <span class="nv">extsz</span><span class="o">=</span>4096   <span class="nv">blocks</span><span class="o">=</span>0, <span class="nv">rtextents</span><span class="o">=</span>0
➜  ~ <span class="nb">mkdir </span>test1
➜  ~ mount /dev/rbd0 test1
<span class="c"># 写入测试</span>
➜  ~ <span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>test1/test-file <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
记录了1+0 的读入
记录了1+0 的写出
1073741824字节<span class="o">(</span>1.1 GB<span class="o">)</span>已复制，16.3689 秒，65.6 MB/秒
➜  ~ <span class="nb">ls </span>test1
test-file
</code></pre></div></div>

<h4 id="24cephfs-测试">2.4、CephFS 测试</h4>

<p>在测试 CephFS 之前需要先创建两个存储池和一个 fs，创建存储池要指定 PG 数量</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph osd pool create cephfs_data 32
ceph osd pool create cephfs_metadata 32
ceph fs new testfs cephfs_metadata cephfs_data
</code></pre></div></div>

<p><strong>PG 概念:</strong></p>

<blockquote>
  <p>当 Ceph 集群接收到存储请求时，Ceph 会将其分散到各个 PG 中，PG 是一组对象的逻辑集合；根据 Ceph 存储池的复制级别，每个 PG的数据会被复制并分发到集群的多个 OSD 上；一般来说增加 PG 数量能降低 OSD 负载，一般每个 OSD 大约分配 50 ~ 100 PG，<strong>关于 PG 数量一般遵循以下公式</strong></p>
</blockquote>

<ul>
  <li>集群 PG 总数 = (OSD 总数 * 100) / 数据最大副本数</li>
  <li>单个存储池 PG 数 = (OSD 总数 * 100) / 数据最大副本数 /存储池数</li>
</ul>

<p><strong>注意，PG 最终结果应当为最接近以上计算公式的 2 的 N 次幂(向上取值)；如我的虚拟机环境每个存储池 PG 数 = <code class="highlighter-rouge">3(OSD) * 100 / 3(副本) / 5(大约 5 个存储池) = 20</code>，向上取 2 的 N 次幂 为 32</strong></p>

<p>挂载 CephFS 一般有两种方式，一种是使用内核驱动挂载，一种是使用 <code class="highlighter-rouge">ceph-fuse</code> 用户空间挂载；内核方式挂载需要提取 Ceph 管理 key，如下</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># key 在 ceph.conf 中</span>
<span class="nb">echo</span> <span class="s2">"AQB37CZZblBkDRAAUrIrRGsHj/NqdKmVlMQ7ww=="</span> <span class="o">&gt;</span> ceph-key
<span class="c"># 创建目录挂载</span>
<span class="nb">mkdir </span>test2
mount <span class="nt">-t</span> ceph 192.168.1.11:6789:/ /root/test2 <span class="nt">-o</span> <span class="nv">name</span><span class="o">=</span>admin,secretfile<span class="o">=</span>ceph-key
<span class="c"># 写入测试</span>
➜  ~ <span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>test2/testFs <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
记录了1+0 的读入
记录了1+0 的写出
1073741824字节<span class="o">(</span>1.1 GB<span class="o">)</span>已复制，6.83251 秒，157 MB/秒
</code></pre></div></div>

<p>使用 ceph-fuse 用户空间挂载方式比较简单，但需要先安装 <code class="highlighter-rouge">ceph-fuse</code> 软件包</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 安装 ceph-fuse</span>
yum <span class="nb">install</span> <span class="nt">-y</span> ceph-fuse
<span class="c"># 挂载</span>
<span class="nb">mkdir </span>test3
ceph-fuse <span class="nt">-m</span> 192.168.1.11:6789 test3
<span class="c"># 写入测试</span>
➜  ~ <span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>test3/testFs <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
记录了1+0 的读入
记录了1+0 的写出
1073741824字节<span class="o">(</span>1.1 GB<span class="o">)</span>已复制，8.18417 秒，131 MB/秒
</code></pre></div></div>

<h4 id="25对象网关测试">2.5、对象网关测试</h4>

<p>对象网关在 <strong>1.5、其他组件创建</strong> 部分已经做了创建(RGW)，此时直接访问 <code class="highlighter-rouge">http://ceph-node-ip:7480</code> 返回如下</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;ListAllMyBucketsResult</span> <span class="na">xmlns=</span><span class="s">"http://s3.amazonaws.com/doc/2006-03-01/"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;Owner&gt;</span>
        <span class="nt">&lt;ID&gt;</span>anonymous<span class="nt">&lt;/ID&gt;</span>
        <span class="nt">&lt;DisplayName/&gt;</span>
    <span class="nt">&lt;/Owner&gt;</span>
    <span class="nt">&lt;Buckets/&gt;</span>
<span class="nt">&lt;/ListAllMyBucketsResult&gt;</span>
</code></pre></div></div>

<p>这就说明网关已经 ok，由于手里没有能读写测试工具，这里暂不做过多说明</p>

<p><strong>本文主要参考 <a href="http://docs.ceph.com/docs/master/start/">Ceph 官方文档 Quick Start</a> 部分，如有其它未说明到的细节可从官方文档获取</strong></p>

<p>转载请注明出处，本文采用 <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/">CC4.0</a> 协议授权</p>
:ET